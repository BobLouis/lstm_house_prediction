{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the CSV file\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Display the first few rows to understand the data\n",
    "print(\"Initial data preview:\")\n",
    "print(df.head())\n",
    "\n",
    "# Drop the \"Address\" column since it is not used in our prediction model\n",
    "df = df.drop(columns=['Address'])\n",
    "\n",
    "\n",
    "# Quick information on the dataframe structure\n",
    "print(\"\\nDataframe info:\")\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data Preprocessing and Feature Engineering for predicting UnitPrice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Remove leading and trailing whitespace from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "print(\"Columns after stripping:\", df.columns.tolist())\n",
    "# Filter the data to keep only rows where Primary Use equals 3\n",
    "df = df[df['Primary Use'] == 3]\n",
    "print(\"Shape after filtering to Primary Use == 3:\", df.shape)\n",
    "\n",
    "\n",
    "# --- Drop rows where the 'Floors' column contains a comma ---\n",
    "# This filters out rows with values like \"1,2,3\" in the 'Floors' column.\n",
    "df = df[~df['Floors'].astype(str).str.contains(',')]\n",
    "print(\"Shape after dropping rows with comma in 'Floors':\", df.shape)\n",
    "\n",
    "# --- Replace Blanks and Placeholders ---\n",
    "df.replace({'--': np.nan, '': np.nan}, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# --- Convert ROC Date to Gregorian Timestamp ---\n",
    "def convert_roc_date(roc_date_str):\n",
    "    try:\n",
    "        parts = roc_date_str.strip().split('/')\n",
    "        if len(parts) == 3:\n",
    "            roc_year = int(parts[0].strip())\n",
    "            month = int(parts[1].strip())\n",
    "            day = int(parts[2].strip())\n",
    "            if month < 1 or month > 12:\n",
    "                raise ValueError(f\"Month {month} out of valid range in date: {roc_date_str}\")\n",
    "            gregorian_year = roc_year + 1911\n",
    "            return pd.Timestamp(year=gregorian_year, month=month, day=day)\n",
    "        else:\n",
    "            print(f\"Unexpected format for date: {roc_date_str}\")\n",
    "            return pd.NaT\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting date {roc_date_str}: {e}\")\n",
    "        return pd.NaT\n",
    "\n",
    "# Convert the Date column\n",
    "df['Date'] = df['Date'].apply(convert_roc_date)\n",
    "print(\"Converted dates:\")\n",
    "print(df['Date'].head())\n",
    "\n",
    "# --- Sort DataFrame by Date ---\n",
    "df.sort_values('Date', inplace=True)\n",
    "\n",
    "# For BuildingType and Primary Use, convert to numeric if necessary.\n",
    "df['BuildingType'] = pd.to_numeric(df['BuildingType'], errors='coerce')\n",
    "df['Primary Use'] = pd.to_numeric(df['Primary Use'], errors='coerce')\n",
    "\n",
    "print(\"\\nMissing values after cleaning numeric columns:\")\n",
    "print(df[['MainBuildingRatio', 'BuildingType', 'Primary Use']].isnull().sum())\n",
    "\n",
    "# --- Identify Features and Target ---\n",
    "# Drop Address (non-numeric) and Date (used for ordering) along with target UnitPrice\n",
    "features = [col for col in df.columns if col not in ['Address', 'UnitPrice', 'Date']]\n",
    "target = 'UnitPrice'\n",
    "\n",
    "# --- Scaling the Features and Target ---\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_target = MinMaxScaler()\n",
    "\n",
    "df_features_scaled = scaler_features.fit_transform(df[features])\n",
    "df_target_scaled = scaler_target.fit_transform(df[[target]])\n",
    "\n",
    "print(\"\\nScaled features shape:\", df_features_scaled.shape)\n",
    "print(\"Scaled target shape:\", df_target_scaled.shape)\n",
    "\n",
    "# --- Create Sequences for LSTM Input ---\n",
    "def create_sequences(features_data, target_data, window_size=5):\n",
    "    X, y = [], []\n",
    "    for i in range(len(features_data) - window_size):\n",
    "        X.append(features_data[i:(i + window_size)])\n",
    "        y.append(target_data[i + window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "window_size = 5\n",
    "X, y = create_sequences(df_features_scaled, df_target_scaled, window_size=window_size)\n",
    "\n",
    "print(\"\\nInput sequence shape (samples, timesteps, features):\", X.shape)\n",
    "print(\"Output sequence shape (samples, target dimension):\", y.shape)\n",
    "\n",
    "# One-hot encode the BuildingType and Primary Use columns\n",
    "df = pd.get_dummies(df, columns=['BuildingType', 'Primary Use'], prefix=['BT', 'PU'])\n",
    "\n",
    "# After encoding, verify the new columns\n",
    "print(\"Columns after one-hot encoding:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model construct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the shape of the input data\n",
    "input_shape = (X.shape[1], X.shape[2])  # (timesteps, features)\n",
    "print(\"Input shape to the model:\", input_shape)\n",
    "\n",
    "# Build the LSTM Model\n",
    "model = Sequential()\n",
    "# First LSTM layer, returns sequences for the next LSTM layer.\n",
    "model.add(LSTM(units=50, activation='tanh', return_sequences=True, input_shape=input_shape))\n",
    "model.add(Dropout(0.2))\n",
    "# Second LSTM layer; no need to return sequences since it's the last LSTM.\n",
    "model.add(LSTM(units=50, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "# Final Dense layer for regression output\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Step 4: Train the LSTM Model ---\n",
    "\n",
    "# Split the dataset into training and validation sets.\n",
    "# For time series data, it's important not to shuffle to maintain the temporal relationship.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape, y_val.shape)\n",
    "\n",
    "# Define an EarlyStopping callback to prevent overfitting.\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model.\n",
    "# Adjust epochs and batch_size as needed.\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Plot Training and Validation Loss ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --- Evaluate the Model on the Validation Set ---\n",
    "val_loss = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(\"Validation Loss (MSE):\", val_loss)\n",
    "\n",
    "# --- Make Predictions on the Validation Set ---\n",
    "y_pred_scaled = model.predict(X_val)\n",
    "\n",
    "# Inverse transform the scaled predictions and validation targets to their original scale\n",
    "y_pred = scaler_target.inverse_transform(y_pred_scaled)\n",
    "y_actual = scaler_target.inverse_transform(y_val)\n",
    "\n",
    "# --- Plot Actual vs. Predicted UnitPrice ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_actual, label='Actual UnitPrice', marker='o', linestyle='-', markersize=3)\n",
    "plt.plot(y_pred, label='Predicted UnitPrice', marker='x', linestyle='--', markersize=3)\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('UnitPrice')\n",
    "plt.title('Actual vs. Predicted UnitPrice on Validation Set')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Compute Mean Squared Error (MSE)\n",
    "mse_val = mean_squared_error(y_actual, y_pred)\n",
    "# Compute Mean Absolute Error (MAE)\n",
    "mae_val = mean_absolute_error(y_actual, y_pred)\n",
    "# Compute R² Score (coefficient of determination)\n",
    "r2_val = r2_score(y_actual, y_pred)\n",
    "\n",
    "print(\"Validation Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_val:.3f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_val:.3f}\")\n",
    "print(f\"R² Score: {r2_val:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
